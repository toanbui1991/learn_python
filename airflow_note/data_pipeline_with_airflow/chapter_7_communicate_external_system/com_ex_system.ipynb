{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import io\n",
    "import pickle\n",
    "\n",
    "import airflow.utils.dates\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "from airflow.providers.amazon.aws.operators.s3_copy_object import S3CopyObjectOperator\n",
    "from airflow.providers.amazon.aws.operators.sagemaker_endpoint import (\n",
    "    SageMakerEndpointOperator,\n",
    ")\n",
    "from airflow.providers.amazon.aws.operators.sagemaker_training import (\n",
    "    SageMakerTrainingOperator,\n",
    ")\n",
    "from sagemaker.amazon.common import write_numpy_to_dense_tensor\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"chapter9_aws_handwritten_digit_classifier\",\n",
    "    schedule_interval=None,\n",
    "    start_date=airflow.utils.dates.days_ago(3),\n",
    ")\n",
    "\n",
    "download_mnist_data = S3CopyObjectOperator(\n",
    "    task_id=\"download_mnist_data\",\n",
    "    source_bucket_name=\"sagemaker-sample-data-eu-west-1\",\n",
    "    source_bucket_key=\"algorithms/kmeans/mnist/mnist.pkl.gz\",\n",
    "    dest_bucket_name=\"your-bucket\",\n",
    "    dest_bucket_key=\"mnist.pkl.gz\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "def _extract_mnist_data():\n",
    "    s3hook = S3Hook()\n",
    "\n",
    "    # Download S3 dataset into memory\n",
    "    mnist_buffer = io.BytesIO()\n",
    "    mnist_obj = s3hook.get_key(bucket_name=\"your-bucket\", key=\"mnist.pkl.gz\")\n",
    "    mnist_obj.download_fileobj(mnist_buffer)\n",
    "\n",
    "    # Unpack gzip file, extract dataset, convert to dense tensor, upload back to S3\n",
    "    mnist_buffer.seek(0)\n",
    "    with gzip.GzipFile(fileobj=mnist_buffer, mode=\"rb\") as f:\n",
    "        train_set, _, _ = pickle.loads(f.read(), encoding=\"latin1\")\n",
    "        output_buffer = io.BytesIO()\n",
    "        write_numpy_to_dense_tensor(\n",
    "            file=output_buffer, array=train_set[0], labels=train_set[1]\n",
    "        )\n",
    "        output_buffer.seek(0)\n",
    "        s3hook.load_file_obj(\n",
    "            output_buffer, key=\"mnist_data\", bucket_name=\"your-bucket\", replace=True\n",
    "        )\n",
    "\n",
    "\n",
    "extract_mnist_data = PythonOperator(\n",
    "    task_id=\"extract_mnist_data\", python_callable=_extract_mnist_data, dag=dag\n",
    ")\n",
    "\n",
    "sagemaker_train_model = SageMakerTrainingOperator(\n",
    "    task_id=\"sagemaker_train_model\",\n",
    "    config={\n",
    "        \"TrainingJobName\": \"mnistclassifier-{{ execution_date.strftime('%Y-%m-%d-%H-%M-%S') }}\",\n",
    "        \"AlgorithmSpecification\": {\n",
    "            \"TrainingImage\": \"438346466558.dkr.ecr.eu-west-1.amazonaws.com/kmeans:1\",\n",
    "            \"TrainingInputMode\": \"File\",\n",
    "        },\n",
    "        \"HyperParameters\": {\"k\": \"10\", \"feature_dim\": \"784\"},\n",
    "        \"InputDataConfig\": [\n",
    "            {\n",
    "                \"ChannelName\": \"train\",\n",
    "                \"DataSource\": {\n",
    "                    \"S3DataSource\": {\n",
    "                        \"S3DataType\": \"S3Prefix\",\n",
    "                        \"S3Uri\": \"s3://your-bucket/mnist_data\",\n",
    "                        \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                    }\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        \"OutputDataConfig\": {\"S3OutputPath\": \"s3://your-bucket/mnistclassifier-output\"},\n",
    "        \"ResourceConfig\": {\n",
    "            \"InstanceType\": \"ml.c4.xlarge\",\n",
    "            \"InstanceCount\": 1,\n",
    "            \"VolumeSizeInGB\": 10,\n",
    "        },\n",
    "        \"RoleArn\": (\n",
    "            \"arn:aws:iam::297623009465:role/service-role/\"\n",
    "            \"AmazonSageMaker-ExecutionRole-20180905T153196\"\n",
    "        ),\n",
    "        \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 24 * 60 * 60},\n",
    "    },\n",
    "    wait_for_completion=True,\n",
    "    print_log=True,\n",
    "    check_interval=10,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "sagemaker_deploy_model = SageMakerEndpointOperator(\n",
    "    task_id=\"sagemaker_deploy_model\",\n",
    "    operation=\"update\",\n",
    "    wait_for_completion=True,\n",
    "    config={\n",
    "        \"Model\": {\n",
    "            \"ModelName\": \"mnistclassifier-{{ execution_date.strftime('%Y-%m-%d-%H-%M-%S') }}\",\n",
    "            \"PrimaryContainer\": {\n",
    "                \"Image\": \"438346466558.dkr.ecr.eu-west-1.amazonaws.com/kmeans:1\",\n",
    "                \"ModelDataUrl\": (\n",
    "                    \"s3://your-bucket/mnistclassifier-output/mnistclassifier\"\n",
    "                    \"-{{ execution_date.strftime('%Y-%m-%d-%H-%M-%S') }}/\"\n",
    "                    \"output/model.tar.gz\"\n",
    "                ),  # this will link the model and the training job\n",
    "            },\n",
    "            \"ExecutionRoleArn\": (\n",
    "                \"arn:aws:iam::297623009465:role/service-role/\"\n",
    "                \"AmazonSageMaker-ExecutionRole-20180905T153196\"\n",
    "            ),\n",
    "        },\n",
    "        \"EndpointConfig\": {\n",
    "            \"EndpointConfigName\": \"mnistclassifier-{{ execution_date.strftime('%Y-%m-%d-%H-%M-%S') }}\",\n",
    "            \"ProductionVariants\": [\n",
    "                {\n",
    "                    \"InitialInstanceCount\": 1,\n",
    "                    \"InstanceType\": \"ml.t2.medium\",\n",
    "                    \"ModelName\": \"mnistclassifier\",\n",
    "                    \"VariantName\": \"AllTraffic\",\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        \"Endpoint\": {\n",
    "            \"EndpointConfigName\": \"mnistclassifier-{{ execution_date.strftime('%Y-%m-%d-%H-%M-%S') }}\",\n",
    "            \"EndpointName\": \"mnistclassifier\",\n",
    "        },\n",
    "    },\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "download_mnist_data >> extract_mnist_data >> sagemaker_train_model >> sagemaker_deploy_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import io\n",
    "import os\n",
    "\n",
    "from airflow.models import BaseOperator\n",
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
    "from airflow.utils.decorators import apply_defaults\n",
    "\n",
    "#write custom operator\n",
    "class PostgresToS3Operator(BaseOperator):\n",
    "    template_fields = (\"_query\", \"_s3_key\")\n",
    "\n",
    "    @apply_defaults\n",
    "    def __init__(\n",
    "        self, postgres_conn_id, query, s3_conn_id, s3_bucket, s3_key, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self._postgres_conn_id = postgres_conn_id\n",
    "        self._query = query\n",
    "        self._s3_conn_id = s3_conn_id\n",
    "        self._s3_bucket = s3_bucket\n",
    "        self._s3_key = s3_key\n",
    "\n",
    "    def execute(self, context):\n",
    "        postgres_hook = PostgresHook(postgres_conn_id=self._postgres_conn_id)\n",
    "        s3_hook = S3Hook(aws_conn_id=self._s3_conn_id)\n",
    "\n",
    "        with postgres_hook.get_cursor() as cursor:\n",
    "            cursor.execute(self._query)\n",
    "            results = cursor.fetchall()\n",
    "            headers = [_[0] for _ in cursor.description]\n",
    "\n",
    "        data_buffer = io.StringIO()\n",
    "        csv_writer = csv.writer(\n",
    "            data_buffer, quoting=csv.QUOTE_ALL, lineterminator=os.linesep\n",
    "        )\n",
    "        csv_writer.writerow(headers)\n",
    "        csv_writer.writerows(results)\n",
    "        data_buffer_binary = io.BytesIO(data_buffer.getvalue().encode())\n",
    "\n",
    "        s3_hook.load_file_obj(\n",
    "            file_obj=data_buffer_binary,\n",
    "            bucket_name=self._s3_bucket,\n",
    "            key=self._s3_key,\n",
    "            replace=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from airflow import DAG\n",
    "from airflow.hooks.base import BaseHook\n",
    "from airflow.operators.python import PythonOperator\n",
    "from custom.postgres_to_s3_operator import PostgresToS3Operator\n",
    "from minio import Minio\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"chapter7_insideairbnb\",\n",
    "    start_date=datetime(2015, 4, 5),\n",
    "    end_date=datetime(2019, 12, 7),\n",
    "    schedule_interval=\"@monthly\",\n",
    ")\n",
    "\n",
    "download_from_postgres = PostgresToS3Operator(\n",
    "    task_id=\"download_from_postgres\",\n",
    "    postgres_conn_id=\"inside_airbnb\",\n",
    "    query=\"SELECT * FROM listings WHERE download_date BETWEEN '{{ prev_ds }}' AND '{{ ds }}'\",\n",
    "    s3_conn_id=\"locals3\",\n",
    "    s3_bucket=\"inside-airbnb\",\n",
    "    s3_key=\"listing-{{ ds }}.csv\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "def _crunch_numbers():\n",
    "    s3_conn = BaseHook.get_connection(\"locals3\")\n",
    "    client = Minio(\n",
    "        s3_conn.extra_dejson[\"host\"].replace(\"http://\", \"\"),\n",
    "        access_key=s3_conn.login,\n",
    "        secret_key=s3_conn.password,\n",
    "        secure=False,\n",
    "    )\n",
    "\n",
    "    # Get list of all objects\n",
    "    objects = [\n",
    "        obj.object_name\n",
    "        for obj in client.list_objects(bucket_name=\"inside-airbnb\", prefix=\"listing\")\n",
    "    ]\n",
    "    df = pd.DataFrame()\n",
    "    for obj in objects:\n",
    "        response = client.get_object(bucket_name=\"inside-airbnb\", object_name=obj)\n",
    "        temp_df = pd.read_csv(\n",
    "            io.BytesIO(response.read()),\n",
    "            usecols=[\"id\", \"price\", \"download_date\"],\n",
    "            parse_dates=[\"download_date\"],\n",
    "        )\n",
    "        df = df.append(temp_df)\n",
    "\n",
    "    # Per id, get the price increase/decrease\n",
    "    # There's probably a nicer way to do this\n",
    "    min_max_per_id = (\n",
    "        df.groupby([\"id\"])\n",
    "        .agg(\n",
    "            download_date_min=(\"download_date\", \"min\"),\n",
    "            download_date_max=(\"download_date\", \"max\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    df_with_min = (\n",
    "        pd.merge(\n",
    "            min_max_per_id,\n",
    "            df,\n",
    "            how=\"left\",\n",
    "            left_on=[\"id\", \"download_date_min\"],\n",
    "            right_on=[\"id\", \"download_date\"],\n",
    "        )\n",
    "        .rename(columns={\"price\": \"oldest_price\"})\n",
    "        .drop(\"download_date\", axis=1)\n",
    "    )\n",
    "    df_with_max = (\n",
    "        pd.merge(\n",
    "            df_with_min,\n",
    "            df,\n",
    "            how=\"left\",\n",
    "            left_on=[\"id\", \"download_date_max\"],\n",
    "            right_on=[\"id\", \"download_date\"],\n",
    "        )\n",
    "        .rename(columns={\"price\": \"latest_price\"})\n",
    "        .drop(\"download_date\", axis=1)\n",
    "    )\n",
    "\n",
    "    df_with_max = df_with_max[\n",
    "        df_with_max[\"download_date_max\"] != df_with_max[\"download_date_min\"]\n",
    "    ]\n",
    "    df_with_max[\"price_diff_per_day\"] = (\n",
    "        df_with_max[\"latest_price\"] - df_with_max[\"oldest_price\"]\n",
    "    ) / ((df_with_max[\"download_date_max\"] - df_with_max[\"download_date_min\"]).dt.days)\n",
    "    df_with_max[[\"price_diff_per_day\"]] = df_with_max[[\"price_diff_per_day\"]].apply(\n",
    "        pd.to_numeric\n",
    "    )\n",
    "    biggest_increase = df_with_max.nlargest(5, \"price_diff_per_day\")\n",
    "    biggest_decrease = df_with_max.nsmallest(5, \"price_diff_per_day\")\n",
    "\n",
    "    # We found the top 5, write back the results.\n",
    "    biggest_increase_json = biggest_increase.to_json(orient=\"records\")\n",
    "    print(f\"Biggest increases: {biggest_increase_json}\")\n",
    "    biggest_increase_bytes = biggest_increase_json.encode(\"utf-8\")\n",
    "    client.put_object(\n",
    "        bucket_name=\"inside-airbnb\",\n",
    "        object_name=\"results/biggest_increase.json\",\n",
    "        data=io.BytesIO(biggest_increase_bytes),\n",
    "        length=len(biggest_increase_bytes),\n",
    "    )\n",
    "\n",
    "    biggest_decrease_json = biggest_decrease.to_json(orient=\"records\")\n",
    "    print(f\"Biggest decreases: {biggest_decrease_json}\")\n",
    "    biggest_decrease_bytes = biggest_decrease_json.encode(\"utf-8\")\n",
    "    client.put_object(\n",
    "        bucket_name=\"inside-airbnb\",\n",
    "        object_name=\"results/biggest_decrease.json\",\n",
    "        data=io.BytesIO(biggest_decrease_bytes),\n",
    "        length=len(biggest_decrease_bytes),\n",
    "    )\n",
    "\n",
    "\n",
    "crunch_numbers = PythonOperator(\n",
    "    task_id=\"crunch_numbers\", python_callable=_crunch_numbers, dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "download_from_postgres >> crunch_numbers"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
