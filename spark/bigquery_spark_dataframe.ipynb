{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using SparkSession to make spark session instance\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "  .appName('1.1. BigQuery Storage & Spark DataFrames - Python')\\\n",
    "  .config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest.jar') \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from bigquery table with read and then config with format\n",
    "#check the table format with printSchema\n",
    "#we can use option method to specify operation like select, filter.\n",
    "table = \"bigquery-public-data.wikipedia.pageviews_2020\"\n",
    "df_wiki_pageviews = spark.read \\\n",
    "  .format(\"bigquery\") \\\n",
    "  .option(\"table\", table) \\\n",
    "  .option(\"filter\", \"datehour >= '2020-03-01' AND datehour < '2020-03-02'\") \\\n",
    "  .load()\n",
    "\n",
    "df_wiki_pageviews.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data frame operstion method like select, where, cache to store dataframe into ram instead of recompute\n",
    "df_wiki_en = df_wiki_pageviews \\\n",
    "  .select(\"title\", \"wiki\", \"views\") \\\n",
    "  .where(\"views > 1000 AND wiki in ('en', 'en.m')\") \\\n",
    "  .cache()\n",
    "\n",
    "df_wiki_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "#given the cache dataframe computer aggreate data with method like groupBy, and agg\n",
    "#get function from pyspark.sql.functions as F\n",
    "#aggregated and then order by that aggregate result with method orderBy.\n",
    "df_wiki_en_totals = df_wiki_en \\\n",
    ".groupBy(\"title\") \\\n",
    ".agg(F.sum('views').alias('total_views'))\n",
    "\n",
    "df_wiki_en_totals.orderBy('total_views', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to write spark dataframe to bigquery table we need to specify: gsc bucket, bigquery dataset, table\n",
    "# specify gcs bucket.\n",
    "gcs_bucket = 'dataproc-bucket-name'\n",
    "#specify data_set and table\n",
    "bq_dataset = 'dataset_name'\n",
    "bq_table = 'wiki_total_pageviews'\n",
    "\n",
    "df_wiki_en_totals.write \\\n",
    "  .format(\"bigquery\") \\\n",
    "  .option(\"table\",\"{}.{}\".format(bq_dataset, bq_table)) \\\n",
    "  .option(\"temporaryGcsBucket\", gcs_bucket) \\\n",
    "  .mode('overwrite') \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using %%bigquery (bigquery magic syntax) to write query directly from jupiter notebook\n",
    "%%bigquery\n",
    "SELECT title, total_views\n",
    "FROM dataset_name.wiki_total_pageviews\n",
    "ORDER BY total_views DESC\n",
    "LIMIT 10"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
