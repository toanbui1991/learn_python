{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "  .appName('1.2. BigQuery Storage & Spark SQL - Python')\\\n",
    "  .config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest.jar') \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read method will return data frame\n",
    "table = \"bigquery-public-data.wikipedia.pageviews_2020\"\n",
    "df_wiki_pageviews = spark.read \\\n",
    "  .format(\"bigquery\") \\\n",
    "  .option(\"table\", table) \\\n",
    "  .option(\"filter\", \"datehour >= '2020-03-01' AND datehour < '2020-03-02'\") \\\n",
    "  .load()\n",
    "\n",
    "df_wiki_pageviews.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with the tempview we can use spark.sql method.\n",
    "#spark.sql turn dataframe\n",
    "#create temp table with method df.createOrReplaceTempView\n",
    "#to use spark.sql interface we have to create\n",
    "df_wiki_pageviews.createOrReplaceTempView(\"wiki_pageviews\")\n",
    "df_wiki_en = spark.sql(\"\"\"\n",
    "SELECT \n",
    " title, wiki, views\n",
    "FROM wiki_pageviews\n",
    "WHERE views > 1000 AND wiki in ('en', 'en.m')\n",
    "\"\"\").cache()\n",
    "\n",
    "df_wiki_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to use sql interface we need table or temp table\n",
    "#using spark.sql interface which allow to use write with sql query interface\n",
    "df_wiki_en.createOrReplaceTempView(\"wiki_en\")\n",
    "df_wiki_en_totals = spark.sql(\"\"\"\n",
    "SELECT \n",
    " title, \n",
    " SUM(views) as total_views\n",
    "FROM wiki_en\n",
    "GROUP BY title\n",
    "ORDER BY total_views DESC\n",
    "\"\"\")\n",
    "\n",
    "df_wiki_en_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write data to bigquery. we need: gcs bucket, bigquery dataset, table\n",
    "# Update to your GCS bucket\n",
    "gcs_bucket = 'dataproc-bucket-name'\n",
    "bq_dataset = 'dataset_name'\n",
    "bq_table = 'wiki_total_pageviews'\n",
    "\n",
    "df_wiki_en_totals.write \\\n",
    "  .format(\"bigquery\") \\\n",
    "  .option(\"table\",\"{}.{}\".format(bq_dataset, bq_table)) \\\n",
    "  .option(\"temporaryGcsBucket\", gcs_bucket) \\\n",
    "  .mode('overwrite') \\\n",
    "  .save()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
